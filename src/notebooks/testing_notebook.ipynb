{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file allows for testing the GraphWorld setup with GNN implementations.\n",
    "It is currently set up to test the SSL methods for the JL benchmarker.\n",
    "\n",
    "Through this notebook you can attach a debugger.\n",
    "Note that graph_tool does not work on windows, so we cannot use the graph generators.\n",
    "Instead, we use the standard datasets from PyG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from graph_world.self_supervised_learning.benchmarker import NNNodeBenchmarkerSSL\n",
    "from graph_world.models.basic_gnn import GCN, SuperGAT\n",
    "from torch_geometric.datasets import Planetoid, KarateClub, FakeDataset\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "from graph_world.self_supervised_learning.pretext_tasks.contrastive_based_different_scale import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "dataset = FakeDataset(num_graphs = 1, avg_num_nodes = 100, num_channels = 16, num_classes = 4)[0]\n",
    "dataset = RandomNodeSplit(split = \"random\", num_test = 20, num_val = 20, num_train_per_class=2)(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the last node disconnected (for testing model robustness)\n",
    "node = dataset.num_nodes - 1\n",
    "mask = (dataset.edge_index[0] != node) & (dataset.edge_index[1] != node)\n",
    "dataset.edge_index = dataset.edge_index[:, mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter setup (for cora)\n",
    "benchmark_params = {\n",
    "    'downstream_epochs' : 100,\n",
    "    'pretext_epochs' : 100,\n",
    "    'downstream_lr' : 3e-4,\n",
    "    'pretext_lr' : 3e-4,\n",
    "    'patience': 50\n",
    "}\n",
    "\n",
    "h_params = {\n",
    "    'in_channels' : dataset.x.shape[1],\n",
    "    'hidden_channels' : 128,\n",
    "    'num_layers' : 2,\n",
    "    'dropout' : 0.5,\n",
    "}\n",
    "\n",
    "pretext_params = {\n",
    "    'alpha' : 0.15,\n",
    "    'k' : 8\n",
    "}\n",
    "\n",
    "generator_config = {\n",
    "    'num_clusters' : 4,\n",
    "}\n",
    "\n",
    "pretext_task = SUBGCON\n",
    "training_scheme = 'JL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [2.457749843597412,\n",
       "  2.185495615005493,\n",
       "  2.3042306900024414,\n",
       "  2.1249043941497803,\n",
       "  2.1014928817749023,\n",
       "  2.178771495819092,\n",
       "  2.1937310695648193,\n",
       "  2.155031442642212,\n",
       "  2.088836193084717,\n",
       "  2.2277801036834717,\n",
       "  2.316065788269043,\n",
       "  2.2809855937957764,\n",
       "  2.256649971008301,\n",
       "  2.1206517219543457,\n",
       "  2.2133941650390625,\n",
       "  2.1477510929107666,\n",
       "  2.0851807594299316,\n",
       "  2.1042957305908203,\n",
       "  2.0970468521118164,\n",
       "  2.201967716217041,\n",
       "  2.1924068927764893,\n",
       "  2.123441219329834,\n",
       "  2.091576337814331,\n",
       "  2.1290788650512695,\n",
       "  2.1613638401031494,\n",
       "  2.044262409210205,\n",
       "  2.067103862762451,\n",
       "  1.9859774112701416,\n",
       "  2.1571006774902344,\n",
       "  2.1229281425476074,\n",
       "  2.045536756515503,\n",
       "  1.9637024402618408,\n",
       "  1.9103254079818726,\n",
       "  2.131218433380127,\n",
       "  2.068760395050049,\n",
       "  1.9914883375167847,\n",
       "  1.8868788480758667,\n",
       "  2.119051933288574,\n",
       "  1.7593915462493896,\n",
       "  2.0345709323883057,\n",
       "  1.9444509744644165,\n",
       "  2.1067464351654053,\n",
       "  1.9399598836898804,\n",
       "  1.8789503574371338,\n",
       "  1.8475972414016724,\n",
       "  1.897080659866333,\n",
       "  1.976871371269226,\n",
       "  1.8740178346633911,\n",
       "  1.8418147563934326,\n",
       "  1.955460548400879,\n",
       "  1.735131025314331,\n",
       "  1.8289698362350464,\n",
       "  2.0463194847106934,\n",
       "  1.8706579208374023,\n",
       "  1.7608736753463745,\n",
       "  1.8234952688217163,\n",
       "  1.7185883522033691,\n",
       "  1.8629997968673706,\n",
       "  1.8025321960449219,\n",
       "  1.9361350536346436,\n",
       "  1.6141371726989746,\n",
       "  1.7259544134140015,\n",
       "  1.679991364479065,\n",
       "  1.4038435220718384,\n",
       "  1.627788782119751,\n",
       "  1.7937781810760498,\n",
       "  1.6282602548599243,\n",
       "  1.6171375513076782,\n",
       "  1.7113820314407349,\n",
       "  1.5517034530639648,\n",
       "  1.7007811069488525,\n",
       "  1.6312952041625977,\n",
       "  1.5484464168548584,\n",
       "  1.3990994691848755,\n",
       "  1.7096410989761353],\n",
       " [5.443329195802638,\n",
       "  5.485098608775493,\n",
       "  5.5344893062174645,\n",
       "  5.581049453305641,\n",
       "  5.135494576130419,\n",
       "  5.082153333553108,\n",
       "  4.670799538353071,\n",
       "  4.984422454688422,\n",
       "  4.917436039783071,\n",
       "  4.374957908063826,\n",
       "  3.878439074900772,\n",
       "  3.732334244686551,\n",
       "  2.707650476915654,\n",
       "  2.5860366297419537,\n",
       "  2.4813562972016823,\n",
       "  2.411551732278741,\n",
       "  2.341934024927466,\n",
       "  2.3112791257379803,\n",
       "  1.7661812792920681,\n",
       "  1.6951111061870876,\n",
       "  1.7928523246234551,\n",
       "  2.16245280804656,\n",
       "  2.1365217820072706,\n",
       "  2.121559861236034,\n",
       "  2.112117462916136,\n",
       "  2.117218036755204,\n",
       "  2.1363875101738508,\n",
       "  2.1488271472902696,\n",
       "  2.1490372574410492,\n",
       "  2.1379260928186676,\n",
       "  2.1254188373729663,\n",
       "  2.125316905842273,\n",
       "  2.1362621148192753,\n",
       "  2.1445658053757284,\n",
       "  2.1555357897099685,\n",
       "  2.161674850842631,\n",
       "  2.173170993811474,\n",
       "  2.1847822115127724,\n",
       "  2.2010910522557072,\n",
       "  2.2224814587690034,\n",
       "  2.252086185433128,\n",
       "  2.2927431944626924,\n",
       "  2.338373762676993,\n",
       "  2.8822117995170515,\n",
       "  3.3900462817915065,\n",
       "  3.3936510686672348,\n",
       "  2.911325160306743,\n",
       "  2.8465659360285627,\n",
       "  2.8356745656428886,\n",
       "  2.8432039864193293,\n",
       "  2.878407464062046,\n",
       "  2.8759303977180597,\n",
       "  2.8581330796370503,\n",
       "  2.8452095429029147,\n",
       "  2.8327553319764265,\n",
       "  2.839388457185657,\n",
       "  2.85234343957756,\n",
       "  2.8759984569300543,\n",
       "  2.900287291488038,\n",
       "  2.924819542100872,\n",
       "  3.4491262924613815,\n",
       "  3.4412052509386477,\n",
       "  3.4293224989451048,\n",
       "  3.4246730141502213,\n",
       "  3.4267267037508136,\n",
       "  3.4338632952333947,\n",
       "  3.442176906670376,\n",
       "  3.456579167680464,\n",
       "  3.4790320576073883,\n",
       "  3.519075909606726,\n",
       "  4.122004776903162,\n",
       "  4.163172173454063,\n",
       "  4.203079859165107,\n",
       "  4.288679562000153,\n",
       "  4.431866569424231],\n",
       " [0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5386029411764706,\n",
       "  0.5386029411764706,\n",
       "  0.5376838235294118,\n",
       "  0.5363051470588236,\n",
       "  0.48682598039215685,\n",
       "  0.5254289215686274,\n",
       "  0.5254289215686274,\n",
       "  0.524969362745098,\n",
       "  0.5631127450980392,\n",
       "  0.5626531862745098,\n",
       "  0.5626531862745098,\n",
       "  0.5626531862745098,\n",
       "  0.5120579481792717,\n",
       "  0.4610031512605042,\n",
       "  0.4621192226890757,\n",
       "  0.4598567792501616,\n",
       "  0.4593972204266322,\n",
       "  0.4605132918552036,\n",
       "  0.4605132918552036,\n",
       "  0.4605132918552036,\n",
       "  0.4605132918552036,\n",
       "  0.46097285067873306,\n",
       "  0.46097285067873306,\n",
       "  0.46097285067873306,\n",
       "  0.46097285067873306,\n",
       "  0.46097285067873306,\n",
       "  0.48664249353587585,\n",
       "  0.48483960892049127,\n",
       "  0.48483960892049127,\n",
       "  0.48483960892049127,\n",
       "  0.5105092517776342,\n",
       "  0.50824680833872,\n",
       "  0.5577259750053867,\n",
       "  0.5581855338289161,\n",
       "  0.5184665212238742,\n",
       "  0.5441361640810171,\n",
       "  0.5441361640810171,\n",
       "  0.541873720642103,\n",
       "  0.5405303948502478,\n",
       "  0.5405303948502478,\n",
       "  0.5405303948502478,\n",
       "  0.5409899536737772,\n",
       "  0.5409899536737772,\n",
       "  0.5409899536737772,\n",
       "  0.5409899536737772,\n",
       "  0.5409899536737772,\n",
       "  0.5409899536737772,\n",
       "  0.5432523971126912,\n",
       "  0.5427928382891618,\n",
       "  0.5423332794656324,\n",
       "  0.5150880063563886,\n",
       "  0.5536909475328593,\n",
       "  0.5554938321482439,\n",
       "  0.5554938321482439,\n",
       "  0.5330848685628098,\n",
       "  0.5330848685628098,\n",
       "  0.5330848685628098,\n",
       "  0.48034502262443435,\n",
       "  0.48146109405300586],\n",
       " {'accuracy': 0.25,\n",
       "  'f1_micro': 0.25,\n",
       "  'f1_macro': 0.22045454545454546,\n",
       "  'rocauc_ovr': 0.5230654761904762,\n",
       "  'rocauc_ovo': 0.5230654761904762,\n",
       "  'logloss': 2.299666229276434},\n",
       " {'accuracy': 0.25,\n",
       "  'f1_micro': 0.25,\n",
       "  'f1_macro': 0.18333333333333332,\n",
       "  'rocauc_ovr': 0.5631127450980392,\n",
       "  'rocauc_ovo': 0.5631127450980392,\n",
       "  'logloss': 2.112117462916136})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training. You can attach a debugger to w/e is needed inside train\n",
    "benchmarker = NNNodeBenchmarkerSSL(generator_config=generator_config, model_class=GCN, \n",
    "                benchmark_params=benchmark_params, h_params=h_params,\n",
    "                pretext_task=pretext_task, pretext_params = pretext_params, training_scheme=training_scheme)\n",
    "benchmarker.SetMasks(train_mask=dataset.train_mask, val_mask=dataset.val_mask, test_mask=dataset.test_mask)\n",
    "benchmarker.train(data=dataset, tuning_metric=\"rocauc_ovr\", tuning_metric_is_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7ab25521b35298f0c482908cd2836fcd803c6b8449977de01eb4d32cc96d8dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
