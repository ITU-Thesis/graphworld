{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for aggregating a collection of HPC tasks on GraphWorld\n",
    "Given a type of experiment, this notebook takes all the json result files of a collection of HPC tasks and moves them into a single file in the `processed` directory. It also maintains a summary file in the same folder for all files for the experiment. Finally it loads the result files and prints basic statistics not part of the summary file (see last cell of this file).\n",
    "\n",
    "Set `RAW_DIR` to the raw experiments you want to process.\n",
    "\n",
    "Set `PROCESSED_DIR` to where the processed results should be stored.\n",
    "The processed results will be stored in shards. Each time this notebook is ran, 1 shard is created. E.g. the shard size depends on the contents of the `RAW_DIR`.\n",
    "\n",
    "The processing assumes that the raw results come from our HPC experimental setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(root, run_to_process):\n",
    "  processed = f'{root}/processed'\n",
    "  raw = f'{root}/raw/{run_to_process}'\n",
    "  if not os.path.isdir(raw):\n",
    "    print(raw, 'is not a directory.')\n",
    "    return\n",
    "\n",
    "  PROCESSED_SHARDS = f'{processed}/shards'\n",
    "\n",
    "  if not os.path.exists(PROCESSED_SHARDS):\n",
    "      os.makedirs(PROCESSED_SHARDS)\n",
    "\n",
    "  # Read (existing) summary file for experiment\n",
    "  try:\n",
    "      with open(f'{processed}/summary.json', 'r') as f:\n",
    "          summary = json.load(f)\n",
    "  except FileNotFoundError:\n",
    "      summary = {\n",
    "          'N_GRAPHS': 0,\n",
    "          'N_RUNS': 0,\n",
    "          'RUN_GRAPHS': [],\n",
    "          'RUN_MARG': [],\n",
    "          'RAW_FILES': []\n",
    "      }\n",
    "\n",
    "  if run_to_process in summary['RAW_FILES']:\n",
    "      print(f'WARNING: {run_to_process} has already been processed!')\n",
    "      return\n",
    "\n",
    "  summary['N_RUNS'] += 1\n",
    "  summary['RAW_FILES'] += [run_to_process]\n",
    "\n",
    "  # Here we read the json shards of each HPC task, \n",
    "  # aggregate them and store everything in one file in the processed folder\n",
    "  lines = []\n",
    "  results_file_regex = r'results\\.ndjson-(\\d{5})-of-(\\d{5})'\n",
    "  successful_runs = []\n",
    "\n",
    "  for sub_dir in next(os.walk(raw))[1]:\n",
    "    sub_dir_full = os.path.join(raw, sub_dir)\n",
    "    is_successful = False\n",
    "    result_files = filter(lambda file: re.match(results_file_regex, file), os.listdir(sub_dir_full))\n",
    "    for result_file in result_files:\n",
    "      with open(os.path.join(sub_dir_full, result_file)) as f:\n",
    "        lines.extend(f.readlines())\n",
    "      is_successful = True\n",
    "    if is_successful:\n",
    "      successful_runs += [sub_dir]\n",
    "                  \n",
    "  with open(f'{processed}/shards/{summary[\"N_RUNS\"]}.ndjson', \"w\") as dst:\n",
    "    for line in lines:\n",
    "      dst.write(line) # Write all graph experiments to same file\n",
    "\n",
    "  # Load lines dataframe for printing statistics\n",
    "  records = map(json.loads, lines)\n",
    "  results_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "  # Getting running times\n",
    "  times = []\n",
    "\n",
    "  for task in next(os.walk(raw))[1]:\n",
    "    if not task in successful_runs:\n",
    "      continue\n",
    "    with open(f'{raw}/slurm_{task}.out', 'r') as f:\n",
    "      last_line = lines[-1].split(\" \")[1]\n",
    "      match = re.search(r'\\d+', last_line)\n",
    "\n",
    "      if match:\n",
    "        lines = f.readlines()\n",
    "        times.append(int(match.group()) // 60)\n",
    "      else:\n",
    "          print(f)\n",
    "      \n",
    "  if len(times) == 0:\n",
    "      times = [math.nan]\n",
    "\n",
    "  # Getting basic statistics of raw data\n",
    "  N_GRAPHS = len(results_df)\n",
    "  N_METHODS = len([col for col in results_df if 'encoder_hidden_channels' in col])\n",
    "  N_TASKS = len(next(os.walk(raw))[1])\n",
    "\n",
    "  AVG_TIME = sum(times) / len(times)\n",
    "  MAX_TIME = max(times)\n",
    "  MIN_TIME = min(times)\n",
    "\n",
    "  # Getting methods that have crashed / are skipped\n",
    "  skipped_methods = {}\n",
    "  for s_col in [col for col in results_df if '_skipped' in col]:\n",
    "      count = results_df[s_col].sum()\n",
    "      if count > 0:\n",
    "          skipped_methods.update({s_col.removesuffix('_skipped'): count})\n",
    "\n",
    "  # Update summary file\n",
    "  summary['N_GRAPHS'] += N_GRAPHS\n",
    "  summary['RUN_GRAPHS'].append(N_GRAPHS)\n",
    "  if not 'marginal_param' in results_df.columns:\n",
    "     assert results_df.shape[0] == 0\n",
    "     return\n",
    "     \n",
    "  marg = results_df['marginal_param'].astype(str).unique()\n",
    "  if len(marg) > 1:\n",
    "    summary['RUN_MARG'].append(\"mixed\")\n",
    "  elif len(marg) == 0:\n",
    "    summary['RUN_MARG'].append([])\n",
    "  else:\n",
    "    summary['RUN_MARG'].append(ast.literal_eval(marg[0]))\n",
    "\n",
    "  with open(f'{processed}/summary.json', 'w') as s:\n",
    "    s.write(json.dumps(summary))\n",
    "\n",
    "\n",
    "  # Printing statistics\n",
    "  print('------- Task/Graph statistics -------')\n",
    "  print(f'Total processed tasks: {N_TASKS}')\n",
    "  print(f'Total processed graphs: {N_GRAPHS}')\n",
    "  print(f'Graphs per task: {N_GRAPHS / N_TASKS}')\n",
    "  print(f'Avg task runtime (min): {AVG_TIME} ({AVG_TIME / (N_GRAPHS / N_TASKS)} per graph)')\n",
    "  print(f'Max task runtime (min): {MAX_TIME} ({MAX_TIME / (N_GRAPHS / N_TASKS)} per graph)')\n",
    "  print(f'Min task runtime (min): {MIN_TIME} ({MIN_TIME / (N_GRAPHS / N_TASKS)} per graph)\\n')\n",
    "\n",
    "  print('------- Skipped (crashed) methods -------')\n",
    "  for k,v in skipped_methods.items():\n",
    "      print(f'{k} skipped {v} times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/daen/Thesis/graphworld/results/dgi_easy/raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/daen/Thesis/graphworld/evaluation/process_results.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224461656e2d485043227d/home/daen/Thesis/graphworld/evaluation/process_results.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m run \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdgi_easy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224461656e2d485043227d/home/daen/Thesis/graphworld/evaluation/process_results.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m root \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/home/daen/Thesis/graphworld/results/\u001b[39m\u001b[39m{\u001b[39;00mrun\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224461656e2d485043227d/home/daen/Thesis/graphworld/evaluation/process_results.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m runs \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/home/daen/Thesis/graphworld/results/\u001b[39;49m\u001b[39m{\u001b[39;49;00mrun\u001b[39m}\u001b[39;49;00m\u001b[39m/raw\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/daen/Thesis/graphworld/results/dgi_easy/raw'"
     ]
    }
   ],
   "source": [
    "# mode = '-2-3-marg'\n",
    "# RUN_TO_PROCESS = 'p_to_q_ratio-avg_degree_2'\n",
    "# RAW_DIR = f'/home/data_shares/scara/graphworld/results/mode{mode}/raw/{RUN_TO_PROCESS}'\n",
    "# PROCESSED_DIR = f'/home/data_shares/scara/graphworld/results/mode{mode}/processed'\n",
    "\n",
    "mode = '-2-3'\n",
    "# root = f'/home/data_shares/scara/graphworld/results/mode{mode}'\n",
    "run = 'dgi_easy'\n",
    "root = f'/home/daen/Thesis/graphworld/results/{run}'\n",
    "runs = os.listdir(f'/home/daen/Thesis/graphworld/results/{run}/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Task/Graph statistics -------\n",
      "Total processed tasks: 1\n",
      "Total processed graphs: 2\n",
      "Graphs per task: 2.0\n",
      "Avg task runtime (min): 16.0 (8.0 per graph)\n",
      "Max task runtime (min): 16 (8.0 per graph)\n",
      "Min task runtime (min): 16 (8.0 per graph)\n",
      "\n",
      "------- Skipped (crashed) methods -------\n"
     ]
    }
   ],
   "source": [
    "for run in runs:\n",
    "    process(root, run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
