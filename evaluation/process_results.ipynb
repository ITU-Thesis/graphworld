{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for aggregating a collection of HPC tasks on GraphWorld\n",
    "Given a type of experiment, this notebook takes all the json result files of a collection of HPC tasks and moves them into a single file in the `processed` directory. It also maintains a summary file in the same folder for all files for the experiment. Finally it loads the result files and prints basic statistics not part of the summary file (see last cell of this file).\n",
    "\n",
    "Set `RESULTS` to the experiment you want to process.\n",
    "This assumes that the raw results are stored in `evaluation/results/RESULTS/raw/node_classification`, and come from our HPC experimental setup. The processed results can then be found in `evaluation/results/RESULTS/processed/`.\n",
    "\n",
    "Set `IGNORE_FAILED=True` if all results for a graph should be ignored (delete entire row of result data) if a single method has failed on it.\n",
    "Set `REMOVE_LEARNING_CURVES=True` if the results contain training/learning curves which should be removed (they take up a lot of space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = 'preliminary'\n",
    "IGNORE_FAILED = False # TODO\n",
    "REMOVE_LEARNING_CURVES=True # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "RAW_DIR = f'results/{RESULTS}/raw/node_classification'\n",
    "PROCESSED_DIR = f'results/{RESULTS}/processed'\n",
    "PROCESSED_SHARDS = f'{PROCESSED_DIR}/shards'\n",
    "NSHARDS = 10\n",
    "\n",
    "if not os.path.exists(PROCESSED_SHARDS):\n",
    "    os.makedirs(PROCESSED_SHARDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read (existing) summary file for experiment\n",
    "try:\n",
    "    with open(f'{PROCESSED_DIR}/summary.json', 'r') as f:\n",
    "        summary = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    summary = {\n",
    "        'N_GRAPHS': 0,\n",
    "        'N_RUNS': 0,\n",
    "        'RUN_GRAPHS': []\n",
    "    }\n",
    "\n",
    "summary['N_RUNS'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we read the json shards of each HPC task, \n",
    "# aggregate them and store everything in one file in the processed folder\n",
    "lines = []\n",
    "for sub_dir in next(os.walk(RAW_DIR))[1]:\n",
    "  for shard_idx in range(NSHARDS):\n",
    "    filename = 'results.ndjson-%s-of-%s' % (str(shard_idx).zfill(5), str(NSHARDS).zfill(5))\n",
    "    with open(f'{RAW_DIR}/{sub_dir}/{filename}', 'r') as f:\n",
    "      lines.extend(f.readlines()) # aggregate all shards (collection of graphs)\n",
    "\n",
    "with open(f'{PROCESSED_DIR}/shards/{summary[\"N_RUNS\"]}.ndjson', \"w\") as dst:\n",
    "  for line in lines:\n",
    "    dst.write(line) # Write all graph experiments to same file\n",
    "\n",
    "# Update summary file\n",
    "summary['N_GRAPHS'] += len(lines)\n",
    "summary['RUN_GRAPHS'].append(len(lines))\n",
    "with open(f'{PROCESSED_DIR}/summary.json', 'w') as s:\n",
    "  s.write(json.dumps(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lines dataframe for printing statistics\n",
    "records = map(json.loads, lines)\n",
    "results_df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting running times\n",
    "times = []\n",
    "\n",
    "for task in next(os.walk(RAW_DIR))[1]:\n",
    "  with open(f'{RAW_DIR}/slurm_{task}.out', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    times.append(int(lines[-1].split(\" \")[1]) // 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting basic statistics of raw data\n",
    "N_GRAPHS = len(results_df)\n",
    "N_METHODS = len([col for col in results_df if 'encoder_hidden_channels' in col])\n",
    "N_TASKS = len(next(os.walk(RAW_DIR))[1])\n",
    "\n",
    "AVG_TIME = sum(times) / len(times)\n",
    "MAX_TIME = max(times)\n",
    "MIN_TIME = min(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting methods that have crashed / are skipped\n",
    "skipped_methods = {}\n",
    "for s_col in [col for col in results_df if '_skipped' in col]:\n",
    "    count = results_df[s_col].sum()\n",
    "    if count > 0:\n",
    "        skipped_methods.update({s_col.removesuffix('_skipped'): count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Task/Graph statistics -------\n",
      "Total processed tasks: 100\n",
      "Total processed graphs: 1000\n",
      "Graphs per task: 10.0\n",
      "Avg task runtime (min): 191.6 (19.16 per graph)\n",
      "Max task runtime (min): 259 (25.9 per graph)\n",
      "Min task runtime (min): 136 (13.6 per graph)\n",
      "\n",
      "------- Skipped (crashed) methods -------\n",
      "GCN_EdgeMask_JL skipped 1 times\n",
      "GAT_EdgeMask_JL skipped 6 times\n",
      "GAT_EdgeMask_PF skipped 6 times\n",
      "GAT_EdgeMask_URL skipped 5 times\n",
      "GCN_GBT_JL skipped 4 times\n",
      "GCN_GBT_PF skipped 2 times\n",
      "GCN_GBT_URL skipped 4 times\n",
      "GAT_GBT_JL skipped 4 times\n",
      "GAT_GBT_PF skipped 4 times\n",
      "GAT_GBT_URL skipped 2 times\n",
      "GIN_GBT_PF skipped 1 times\n",
      "GIN_GBT_URL skipped 1 times\n"
     ]
    }
   ],
   "source": [
    "# Printing statistics\n",
    "print('------- Task/Graph statistics -------')\n",
    "print(f'Total processed tasks: {N_TASKS}')\n",
    "print(f'Total processed graphs: {N_GRAPHS}')\n",
    "print(f'Graphs per task: {N_GRAPHS / N_TASKS}')\n",
    "print(f'Avg task runtime (min): {AVG_TIME} ({AVG_TIME / (N_GRAPHS / N_TASKS)} per graph)')\n",
    "print(f'Max task runtime (min): {MAX_TIME} ({MAX_TIME / (N_GRAPHS / N_TASKS)} per graph)')\n",
    "print(f'Min task runtime (min): {MIN_TIME} ({MIN_TIME / (N_GRAPHS / N_TASKS)} per graph)\\n')\n",
    "\n",
    "print('------- Skipped (crashed) methods -------')\n",
    "for k,v in skipped_methods.items():\n",
    "    print(f'{k} skipped {v} times')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
