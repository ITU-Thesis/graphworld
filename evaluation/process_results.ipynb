{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for aggregating a collection of HPC tasks on GraphWorld\n",
    "Given a type of experiment, this notebook takes all the json result files of a collection of HPC tasks and moves them into a single file in the `processed` directory. It also maintains a summary file in the same folder for all files for the experiment. Finally it loads the result files and prints basic statistics not part of the summary file (see last cell of this file).\n",
    "\n",
    "Set `RAW_DIR` to the raw experiments you want to process.\n",
    "\n",
    "Set `PROCESSED_DIR` to where the processed results should be stored.\n",
    "The processed results will be stored in shards. Each time this notebook is ran, 1 shard is created. E.g. the shard size depends on the contents of the `RAW_DIR`.\n",
    "\n",
    "The processing assumes that the raw results come from our HPC experimental setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = f'/home/data_shares/scara/graphworld/results/mode4/raw/node_classification'\n",
    "PROCESSED_DIR = f'/home/data_shares/scara/graphworld/results/mode4/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "\n",
    "PROCESSED_SHARDS = f'{PROCESSED_DIR}/shards'\n",
    "NSHARDS = 10\n",
    "\n",
    "if not os.path.exists(PROCESSED_SHARDS):\n",
    "    os.makedirs(PROCESSED_SHARDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read (existing) summary file for experiment\n",
    "try:\n",
    "    with open(f'{PROCESSED_DIR}/summary.json', 'r') as f:\n",
    "        summary = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    summary = {\n",
    "        'N_GRAPHS': 0,\n",
    "        'N_RUNS': 0,\n",
    "        'RUN_GRAPHS': [],\n",
    "        'RUN_MARG': []\n",
    "    }\n",
    "\n",
    "summary['N_RUNS'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we read the json shards of each HPC task, \n",
    "# aggregate them and store everything in one file in the processed folder\n",
    "lines = []\n",
    "for sub_dir in next(os.walk(RAW_DIR))[1]:\n",
    "  for shard_idx in range(NSHARDS):\n",
    "    filename = 'results.ndjson-%s-of-%s' % (str(shard_idx).zfill(5), str(NSHARDS).zfill(5))\n",
    "    with open(f'{RAW_DIR}/{sub_dir}/{filename}', 'r') as f:\n",
    "      lines.extend(f.readlines()) # aggregate all shards (collection of graphs)\n",
    "\n",
    "with open(f'{PROCESSED_DIR}/shards/{summary[\"N_RUNS\"]}.ndjson', \"w\") as dst:\n",
    "  for line in lines:\n",
    "    dst.write(line) # Write all graph experiments to same file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lines dataframe for printing statistics\n",
    "records = map(json.loads, lines)\n",
    "results_df = pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting running times\n",
    "times = []\n",
    "\n",
    "for task in next(os.walk(RAW_DIR))[1]:\n",
    "  with open(f'{RAW_DIR}/slurm_{task}.out', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    times.append(int(lines[-1].split(\" \")[1]) // 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting basic statistics of raw data\n",
    "N_GRAPHS = len(results_df)\n",
    "N_METHODS = len([col for col in results_df if 'encoder_hidden_channels' in col])\n",
    "N_TASKS = len(next(os.walk(RAW_DIR))[1])\n",
    "\n",
    "AVG_TIME = sum(times) / len(times)\n",
    "MAX_TIME = max(times)\n",
    "MIN_TIME = min(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting methods that have crashed / are skipped\n",
    "skipped_methods = {}\n",
    "for s_col in [col for col in results_df if '_skipped' in col]:\n",
    "    count = results_df[s_col].sum()\n",
    "    if count > 0:\n",
    "        skipped_methods.update({s_col.removesuffix('_skipped'): count})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update summary file\n",
    "summary['N_GRAPHS'] += N_GRAPHS\n",
    "summary['RUN_GRAPHS'].append(N_GRAPHS)\n",
    "marg = results_df['marginal_param'].astype(str).unique()\n",
    "if len(marg) > 1:\n",
    "  summary['RUN_MARG'].append(\"mixed\")\n",
    "elif len(marg) == 0:\n",
    "  summary['RUN_MARG'].append([])\n",
    "else:\n",
    "  summary['RUN_MARG'].append(ast.literal_eval(marg[0]))\n",
    "\n",
    "with open(f'{PROCESSED_DIR}/summary.json', 'w') as s:\n",
    "  s.write(json.dumps(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Task/Graph statistics -------\n",
      "Total processed tasks: 100\n",
      "Total processed graphs: 500\n",
      "Graphs per task: 5.0\n",
      "Avg task runtime (min): 192.71 (38.542 per graph)\n",
      "Max task runtime (min): 430 (86.0 per graph)\n",
      "Min task runtime (min): 12 (2.4 per graph)\n",
      "\n",
      "------- Skipped (crashed) methods -------\n"
     ]
    }
   ],
   "source": [
    "# Printing statistics\n",
    "print('------- Task/Graph statistics -------')\n",
    "print(f'Total processed tasks: {N_TASKS}')\n",
    "print(f'Total processed graphs: {N_GRAPHS}')\n",
    "print(f'Graphs per task: {N_GRAPHS / N_TASKS}')\n",
    "print(f'Avg task runtime (min): {AVG_TIME} ({AVG_TIME / (N_GRAPHS / N_TASKS)} per graph)')\n",
    "print(f'Max task runtime (min): {MAX_TIME} ({MAX_TIME / (N_GRAPHS / N_TASKS)} per graph)')\n",
    "print(f'Min task runtime (min): {MIN_TIME} ({MIN_TIME / (N_GRAPHS / N_TASKS)} per graph)\\n')\n",
    "\n",
    "print('------- Skipped (crashed) methods -------')\n",
    "for k,v in skipped_methods.items():\n",
    "    print(f'{k} skipped {v} times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
